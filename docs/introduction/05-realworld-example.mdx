---
id: realworld-example
title: "Getting some real-world data"
sidebar_label: "Real-world example"
description: Your first steps into the world of scraping with Crawlee
---

import ApiLink from '@site/src/components/ApiLink';

> *Hey, guys, you know, it's cool that we can scrape the `<title>` elements of web pages, but that's not very useful. Can we finally scrape some real data and save it somewhere in a machine-readable format? Because that's why we started reading this tutorial in the first place!*

We hear you, young padawan! First, learn how to crawl, we must. Only then, walk through data, we can!

### Making a store crawler

Fortunately, you don't have to travel to a galaxy far far away to find a good candidate for learning how to scrape structured data. The [Apify Store](https://apify.com/store) is a library of public actors that anyone can grab and use. You can find ready-made solutions for crawling [Google Places](https://apify.com/drobnikj/crawler-google-places), [Amazon](https://apify.com/vaclavrut/amazon-crawler), [Google Search](https://apify.com/apify/google-search-scraper), [Booking](https://apify.com/dtrungtin/booking-scraper), [Instagram](https://apify.com/jaroslavhejlek/instagram-scraper), [Tripadvisor](https://apify.com/maxcopell/tripadvisor) and many other websites. Feel free to check them out! It's also a great place to practice your Jedi scraping skills since it has categories, lists and details.

### The importance of having a plan

Sometimes scraping is really straightforward, but most of the time, it really pays off to do a bit of research first and try to answer some of these questions:
- How is the website structured?
- Can I scrape it only with HTTP requests (read "with `CheerioCrawler`")?
- Would I need a full browser solution?
- Are there any anti-scraping protections in place?
- Do I need to parse the HTML or can I get the data otherwise, such as directly from the website's API?

Jakub, one of Apify's founders, wrote a [great article](https://blog.apify.com/web-scraping-in-2018-forget-html-use-xhrs-metadata-or-javascript-variables-8167f252439c) about all the different techniques, tips and tricks related to web scraping, so make sure to check that out!

For the purposes of this tutorial, let's just go ahead with HTTP requests and HTML parsing using `CheerioCrawler`. The number one reason being: you already know how to use it, and we want to build on that knowledge to learn specific crawling and scraping techniques.

#### Choosing the data you need

A good first step is always to figure out what is it you want to scrape and where to find it. For the time being, let's just agree that we want to scrape all actors from [Apify Store](https://apify.com/store) in all categories (which can be found on the left side of the page) and for each actor we want to get its:

- URL
- Owner
- Unique identifier (such as `apify/web-scraper`)
- Title
- Description
- Last modification date
- Number of runs

You can notice that some information is available directly on the list page, but for details such as "Last modification date" or "Number of runs" we'll also need to open the actor detail pages.

![data to scrape](/img/getting-started/scraping-practice.jpg 'Overview of data to be scraped.')

#### Analyzing the target

Knowing that we will use plain HTTP requests, we immediately know that we won't be able to manipulate the website in any way. We will only be able to go through the HTML it gives us and parse our data from there. This might sound like a huge limitation, but you might be surprised in how effective it can be. Let's get to it!

#### The start URL(s)

This is where you start your crawl. It's convenient to start as close to our data as possible. For example, it wouldn't make much sense to start at `apify.com` and look for a `store` link there, when we already know that everything we want to extract can be found at the `apify.com/store` page.

Once we look at the `apify.com/store` page more carefully, we see that the categories themselves produce URLs that we can use to access those individual categories.

```
https://apify.com/store?category=AUTOMATION
```

Should we write down all the category URLs down and use all of them as start URLs? It's definitely possible, but what if a new category appears on the page later? You would not know about it unless you manually visit the page and inspect it again. So scraping the category links off the store page certainly makes sense. This way you always get an up-to-date list of categories.

But is it really that straightforward? By digging further into the store page's HTML we can eventually find out that it does not actually contain the category links. The menu on the left uses JavaScript to display the items from a given category and, as we've learned earlier, `CheerioCrawler` cannot execute JavaScript. Moreover - the page loads the actors when opened in the browser, but it does not "show" us any when processed with a plain HTTP request.

> We've deliberately chosen this scenario to show an example of the number one weakness of `CheerioCrawler`. We will overcome this difficulty in our `PlaywrightCrawler` tutorial, but at the cost of compute resources and speed. Always remember that no tool is best for everything!

So firstly, let's agree that within the `CheerioCrawler` tutorial we would scrape all public actors made by Apify from the [Apify team account](https://apify.com/apify) page. Secondly, it means that we're back to the pre-selected list of URLs. Since we cannot scrape the list dynamically, we have to manually collect the links and then use them in our crawler. We lose the ability to scrape all the categories/actor details and instead scrape public actors of a specific user, but we keep the low resource consumption and speed advantages of `CheerioCrawler`.

Therefore, we've determined that we should use Apify team account page as a start URL and that it should look as follows:

```
https://apify.com/apify
```

### The crawling strategy

Now that we know where to start, we need to figure out where to go next. Since we've eliminated one level of crawling by selecting a specific user page manually, we now only need to crawl the actor detail pages. The algorithm therefore follows:

1. Visit the user page containing the list of their public actors (our start URL).
2. Enqueue all links to actor details.
3. Visit all actor details and extract data.
4. Repeat 1 - 3 for all start URLs (we have only one in this tutorial, but feel free to try it for more user pages).

> Technically, this is a depth first crawl and the crawler will perform a breadth first crawl by default, but that's an implementation detail. We've chosen this notation since a breadth first crawl would be less readable.

`CheerioCrawler` will make sure to visit the pages for you, if you provide the correct requests, and you already know how to enqueue pages, so this should be fairly easy. Nevertheless, there are two more tricks that we'd like to showcase.

#### Using a `RequestList`

`RequestList` is a perfect tool for scraping a pre-existing list of URLs and if we think about our start URLs, this is exactly what we have! A list of links to the different user pages. Let's see how we'd get them into a `RequestList`.

```js
const sources = [
    'https://apify.com/apify',
];

const requestList = await RequestList.open('users', sources);
```

As you can see, similarly to the `RequestQueue.open()` function, there is a `RequestList.open()` function that will create a `RequestList` instance for you. The first argument is the name of the `RequestList` (or an options object). It is used to persist the crawling state of the list. This is useful when you want to continue where you left off after an error or a process restart. The second argument is the `sources` array, which is nothing more than a list of URLs you wish to crawl.

> `RequestQueue` is a persistent store by default, so no name is needed, while the `RequestList` only lives in memory and giving it a name enables it to become persistent.

You might now want to ask one of these questions:
- Can I enqueue into `RequestList` too?
- How do I make `RequestList` work together with `RequestQueue` since I need the queue to enqueue new requests.

The answer to the first one is a definitive no. `RequestList` is immutable and once you create it, you cannot add or remove requests from it. The answer to the second one is simple. `RequestList` and `RequestQueue` are made to work together out-of-the-box in crawlers, so all you need to do is use them both and the crawlers will do the rest.

```js
const crawler = new CheerioCrawler({
    requestList,
    requestQueue,
    requestHandler,
});
```

> For those wondering how this works, the `RequestList` requests are enqueued into the `RequestQueue` right before their execution and only processed by the `RequestQueue` afterwards.

:::tip Prefer `crawler.addRequests()`

While `RequestList` might be a good fit for some use cases like handling very large input lists, or when you don't need any dynamic enqueuing at all, using `crawler.addRequests()` should be generally the preferred way to handle enqueuing of initial requests. It will enqueue the initial batch of 1000 requests and resolve right after that, so it won't block the start of crawling. Instead, it will continue adding more requests to the queue in batches in the background while the crawler runs.

:::

Following section is here mainly to show how to work with `RequestList`.

#### Sanity check

It's always useful to create some simple boilerplate code to see that you've got everything set up correctly before you start to write the scraping logic itself. You might realize that something in your previous analysis doesn't quite add up, or the website might not behave exactly as you expected.

Let's use your newly acquired `RequestList` knowledge and everything you know from the previous chapters to create a new crawler that'll just visit all the user page URLs we selected and print the text content of all the public actors of the user. Try running the code below in your selected environment. You should see, albeit very badly formatted, the text of the individual public actor cards of the selected user.

```ts
import { CheerioCrawler, RequestList } from 'crawlee';

const sources = [
    'https://apify.com/apify',
];

const requestList = await RequestList.open('users', sources);

const crawler = new CheerioCrawler({
    requestList,
    requestHandler: async ({ $ }) => {
        // Select all the actor cards.
        $('.ActorStoreItem').each((i, el) => {
            const text = $(el).text();
            console.log(`ITEM: ${text}\n`);
        });
    },
});

await crawler.run();
```

You might be wondering how we got that `.ActorStoreItem` selector. After analyzing the user page using a browser's DevTools, we've determined that it's a good selector to select all the currently displayed actor cards. DevTools and CSS selectors are quite a large topic, so we can't go into too much detail now, but here are a few general pointers.

#### DevTools crash course

> We'll use Chrome DevTools here, since it's the most common browser, but feel free to use any other, it's all very similar.

You could pick any user page, but let's just go with Apify team account. Let's open `https://apify.com/apify` in Chrome and open DevTools either by right-clicking anywhere in the page and selecting `Inspect`, or by pressing `F12` or by any other means relevant to our system. Once we're there, we'll see a bunch of DevToolsy stuff and a view of the user's page with the individual public actor cards.

Now, let's find the `Select an element` tool and use it to select one of the actor cards. You need to make sure to select the whole card, not just some of its contents, such as its title or description.

In the resulting HTML display, it will put your cursor somewhere. Inspecting the HTML around it, you'll see that there are some CSS classes attached to the different HTML elements.

By hovering over the individual elements, you will see their placement in the page's view. It's easy to see the page's structure around the actor cards now. All the cards are displayed in a `<div>` with a `data-test` attribute with `actorCard` value, which holds an `<a>` with some computer-generated class names and also with the class of `ActorStoreItem`.

> Yes, there are other HTML elements and other classes too. You can safely ignore them.

It should now make sense how we got that `.ActorStoreItem` selector. It's just a selector that finds all elements that are annotated with the `ActorStoreItem` class and those just happen to be the actor cards only.

It's always a good idea to double-check that though, so go into the DevTools Console and run

```ts
document.querySelectorAll('.ActorStoreItem');
```

You will see that only the actor cards will be returned, and nothing else.

#### Enqueueing the detail links using a custom selector

In the previous chapter, we used the `enqueueLinks()` function like this:

```js
await enqueueLinks();
```

While very useful in that scenario, we need something different now. Instead of finding all the `<a href="..">` targeting the same hostname, we need to find only the specific ones that will take us to the actor detail pages. Otherwise, we'd be visiting a lot of other pages that we're not interested in. Using the power of DevTools and yet another `enqueueLinks()` parameter, this becomes fairly easy.

```ts
import { CheerioCrawlingContext } from 'crawlee';

export async function requestHandler({ request, enqueueLinks }: CheerioCrawlingContext) {
    console.log(`Processing ${request.url}`);

    // Only enqueue new links from the user pages.
    if (!request.userData.detailPage) {
        await enqueueLinks({
            selector: 'div[data-test=actorCard] > a.ActorStoreItem',
            userData: { detailPage: true },
        });
    }
}
```

The code should look pretty familiar to you. It's a very simple `requestHandler` where we log the currently processed URL to the console and enqueue more links. But there are also a few new, interesting additions. Let's break it down.

##### The `selector` parameter of `enqueueLinks()`

When we previously used `enqueueLinks()`, we were not providing any `selector` parameter, and it was fine, because we wanted to use the default value, which is `a` - finds all `<a>` elements. But now, we need to be more specific. There are multiple `<a>` links on the given user page, and we're only interested in those that will take us to item (actor) details. Using the DevTools, we found out that we can select the links we wanted using the `div[data-test=actorCard] > a.ActorStoreItem` selector, which selects all the `<a class="ActorStoreItem ...">` elements. And those are exactly the ones we're interested in.

##### Finally, the `userData` of `enqueueLinks()`

You will see `userData` used often throughout Crawlee, and it's nothing more than a place to store our own data on a `Request` instance. You can access it with `request.userData` and it's a plain `Object` that can be used to store anything that needs to survive the full life-cycle of the `Request`. You can also use `Request.label` shortcut, that under the hood controls the `userData.label`.

To modify all the `Request` instances before enqueueing, you can use the `transformRequestFunction` option of `enqueueLinks()`. In our case, we use it to set a `userData.detailPage` property to the enqueued requests to be able to easily differentiate between the user pages and the detail pages.

#### Another sanity check

It's always good to work step by step. We have this new enqueueing logic in place and since the previous [Sanity check](#sanity-check) worked only with a `RequestList`, because we were not enqueueing anything, don't forget to add back the `RequestQueue` and `maxRequestsPerCrawl` limit. Let's test it out!

```ts
import { CheerioCrawler, RequestList, RequestQueue } from 'crawlee';

const sources = [
    'https://apify.com/apify',
];

const requestList = await RequestList.open('users', sources);
const requestQueue = await RequestQueue.open(); // <---------------------

const crawler = new CheerioCrawler({
    maxRequestsPerCrawl: 20, // <----------------------------------------
    requestList,
    requestQueue, // <---------------------------------------------------
    async requestHandler({ request, enqueueLinks }) {
        console.log(`Processing ${request.url}`);

        // Only enqueue new links from the user pages.
        if (!request.userData.detailPage) {
            await enqueueLinks({
                selector: 'div[data-test=actorCard] > a.ActorStoreItem',
                userData: { detailPage: true },
            });
        }
    },
});

await crawler.run();
```

We've added the `requestHandler()` with the `enqueueLinks()` logic from the previous section to the code we wrote earlier. As always, try running it in the environment of your choice. You should see the crawler logging a number of links to the console, as it crawls the user page first and then all the links to the actor detail pages it found.

This concludes our Crawling strategy section, because we have taught the crawler to visit all the pages we need. Let's continue with scraping the tasty data.

### Scraping data

At the beginning of this chapter, we created a list of the information we wanted to collect about the actors in the store. Let's review that and figure out ways to access it.

- URL
- Owner
- Unique identifier (such as `apify/web-scraper`)
- Title
- Description
- Last modification date
- Number of runs

![data to scrape](/img/getting-started/scraping-practice.jpg 'Overview of data to be scraped.')

#### Scraping the URL, Owner and Unique identifier

Some information is lying right there in front of us without even having to touch the actor detail pages. The `URL` we already have - the `request.url`. And by looking at it carefully, we realize that it already includes the `owner` and the `unique identifier` too. We can just split the `string` and be on our way then!

> You could also use the `request.loadedUrl` instead. Remember the difference: `request.url` is what you enqueue, `request.loadedUrl` is what gets processed (after possible redirects).

```js
// request.url = https://apify.com/apify/web-scraper

const urlArr = request.url.split('/').slice(-2); // ['apify', 'web-scraper']
const uniqueIdentifier = urlArr.join('/'); // 'apify/web-scraper'
const owner = urlArr[0]; // 'apify'
```

> It's always a matter of preference, whether to store this information separately in the resulting dataset, or not. Whoever uses the dataset can easily parse the `owner` from the `URL`, so should we duplicate the data unnecessarily? Our opinion is that unless the increased data consumption would be too large to bear, it's always better to make the dataset as readable as possible. Someone might want to filter for example by `owner` and keeping only the `URL` in the dataset would make this complicated without using additional tools.

#### Scraping Title, Description, Last modification date and Number of runs

Now it's time to add more data to the results. Let's open one of the public actor detail pages, for example the [`apify/web-scraper`](https://apify.com/apify/web-scraper) page and use our DevTools-Fu to figure out how to get the title of the actor.

##### Title

![actor title](/img/getting-started/title.jpg 'Finding actor title in DevTools.')

By using the element selector tool, you can see that the title is there under an `<h1>` tag, as titles should be. However, in some cases there could actually be more `<h1>` tags on the same page. This should get you thinking. Is there any parent element that includes your `<h1>` tag, but not the other ones, i.e. narrows down the matching elements? Yes, there is! There is a `<header>` element that we can use to select only the heading we're interested in. And although in this case it's not really necessary, it's still a good idea to use more specific selectors to make sure the crawler would work as expected even if there will be some changes in the page HTML after some time.

> Remember that you can press CTRL+F (or CMD+F on Mac) in the Elements tab of DevTools to open the search bar where you can quickly search for elements using their selectors. And always make sure to use the DevTools to verify your scraping process and assumptions. It's faster than changing the crawler code all the time.

To get the title we just need to find it using `Cheerio` and a `header h1` selector, which selects all `<h1>` elements that have a `<header>` ancestor. And as we already know, there's only one.

```js
return {
    title: $('header h1').text(),
};
```

##### Description

Getting the actor's description is a little more involved, but still pretty straightforward. You can't just simply search for a `<p>` tag, because there's a lot of them in the page. We need to narrow our search down a little. Using the DevTools we find out that the actor description is nested within the `<header>` element too, same as the title. Moreover, the actual description is nested inside a `<span>` tag with a class `actor-description`.

![actor description selector](/img/getting-started/description.jpg 'Finding actor description in DevTools.')

```js
return {
    title: $('header h1').text(),
    description: $('header span.actor-description').text(),
};
```

##### Last modification date

The DevTools tell us that the `modifiedDate` can be found in the `<time>` element inside `<ul class="ActorHeader-stats">`.

![actor last modification date selector](/img/getting-started/modified-date.jpg 'Finding actor last modification date in DevTools.')

```js
return {
    title: $('header h1').text(),
    description: $('header span.actor-description').text(),
    modifiedDate: new Date(
        Number(
            $('ul.ActorHeader-stats time').attr('datetime'),
        ),
        // or you can do just: new Date(+$('ul.ActorHeader-stats time').attr('datetime'))
    ),
};
```

It might look a little too complex at first glance, but let's walk through it. We find the right `<time>` element, and then we read its `datetime` attribute, because that's where a unix timestamp is stored as a `string`.

But we would much rather see a readable date in our results, not a unix timestamp, so we need to convert it. Unfortunately the `new Date()` constructor will not accept a `string`, so we cast the `string` to a `number` using the `Number()` function before actually calling `new Date()`. Phew!

##### Run count

And so we're finishing up with the `runCount`. There's no specific element like `<time>`, so we need to create a complex selector and then do a transformation on the result.

```js
return {
    title: $('header h1').text(),
    description: $('header span.actor-description').text(),
    modifiedDate: new Date(
        Number(
            $('ul.ActorHeader-stats time').attr('datetime'),
        ),
    ),
    runCount: Number(
        $('ul.ActorHeader-stats > li:nth-of-type(3)')
            .text()
            .match(/[\d,]+/)[0]
            .replaceAll(',', ''),
    ),
};
```

The `ul.ActorHeader-stats > li:nth-of-type(3)` looks complicated, but it only reads that we're looking for a `<ul class="ActorHeader-stats ...">` element and within that element we're looking for the third `<li>` element. We grab its text, but we're only interested in the number of runs. So we parse the number out using a regular expression, but its type is still a `string`, so we finally convert the result to a `number` by wrapping it with a `Number()` call.

> The numbers are formatted with commas as thousands separators (e.g. `'1,234,567'`), so to extract it, we first use regular expression `/[\d,]+/` - it will search for consecutive number or comma characters. Then we extract the match via `.match(/[\d,]+/)[0]` and finally remove the commas by calling `.replaceAll(',', '')`. This will give us a string (e.g. `'1234567'`) that can be converted via `Number` function.

And there we have it! All the data we needed in a single object. For the sake of completeness, let's add the properties we parsed from the URL earlier, and we're good to go.

```js
const urlArr = request.url.split('/').slice(-2);

const results = {
    url: request.url,
    uniqueIdentifier: urlArr.join('/'),
    owner: urlArr[0],
    title: $('header h1').text(),
    description: $('header span.actor-description').text(),
    modifiedDate: new Date(
        Number(
            $('ul.ActorHeader-stats time').attr('datetime'),
        ),
    ),
    runCount: Number(
        $('ul.ActorHeader-stats > li:nth-of-type(3)')
            .text()
            .match(/[\d,]+/)[0]
            .replaceAll(',', ''),
    ),
};

console.log('RESULTS: ', results);
```

#### Trying it out (sanity check #3)

We have everything we need so just grab our newly created scraping logic, dump it into our original `requestHandler()` and see the magic happen!

```js
import { CheerioCrawler, RequestList, RequestQueue } from 'crawlee';

const sources = [
    'https://apify.com/apify',
];

const requestList = await RequestList.open('users', sources);
const requestQueue = await RequestQueue.open();

const crawler = new CheerioCrawler({
    maxRequestsPerCrawl: 20,
    requestList,
    requestQueue,
    async requestHandler({ $, request, enqueueLinks }) {
        console.log(`Processing ${request.url}`);

        // This is our new scraping logic.
        if (request.userData.detailPage) {
            const urlArr = request.url.split('/').slice(-2);

            const results = {
                url: request.url,
                uniqueIdentifier: urlArr.join('/'),
                owner: urlArr[0],
                title: $('header h1').text(),
                description: $('header span.actor-description').text(),
                modifiedDate: new Date(
                    Number(
                        $('ul.ActorHeader-stats time').attr('datetime'),
                    ),
                ),
                runCount: Number(
                    $('ul.ActorHeader-stats > li:nth-of-type(3)')
                        .text()
                        .match(/[\d,]+/)[0]
                        .replaceAll(',', ''),
                ),
            };
            console.log('RESULTS', results);
        }

        // Only enqueue new links from the user pages.
        if (!request.userData.detailPage) {
            await enqueueLinks({
                selector: 'div[data-test=actorCard] > a.ActorStoreItem',
                userData: { detailPage: true },
            });
        }
    },
});

await crawler.run();
```

> Notice again that we're scraping on the detail pages `request.userData.detailPage === true`, but we're only enqueueing on the user pages `request.userData.detailPage === undefined`.

When running the actor in the environment of your choice, you should see the crawled URLs and their scraped data printed to the console.

### Saving the scraped data

A data extraction job would not be complete without saving the data for later use and processing. We've come to the final and most difficult part of this chapter so make sure to pay attention very carefully!

First, replace the `console.log('RESULTS', results)` call with

```js
import { Dataset } from 'crawlee';

await Dataset.pushData(results);
```

and that's it. Unlike in the previous paragraph, we are being serious now. That's it, we're done. The final code therefore looks exactly like this:

```js
import { CheerioCrawler, RequestList, RequestQueue, Dataset } from 'crawlee';

const sources = [
    'https://apify.com/apify',
];

const requestList = await RequestList.open('users', sources);
const requestQueue = await RequestQueue.open();

const crawler = new CheerioCrawler({
    maxRequestsPerCrawl: 20,
    requestList,
    requestQueue,
    async requestHandler({ $, request, enqueueLinks }) {
        console.log(`Processing ${request.url}`);

        // This is our new scraping logic.
        if (request.userData.detailPage) {
            const urlArr = request.url.split('/').slice(-2);

            const results = {
                url: request.url,
                uniqueIdentifier: urlArr.join('/'),
                owner: urlArr[0],
                title: $('header h1').text(),
                description: $('header span.actor-description').text(),
                modifiedDate: new Date(
                    Number(
                        $('ul.ActorHeader-stats time').attr('datetime'),
                    ),
                ),
                runCount: Number(
                    $('ul.ActorHeader-stats > li:nth-of-type(3)')
                        .text()
                        .match(/[\d,]+/)[0]
                        .replaceAll(',', ''),
                ),
            };
            await Dataset.pushData(results);
        }

        // Only enqueue new links from the user pages.
        if (!request.userData.detailPage) {
            await enqueueLinks({
                selector: 'div[data-test=actorCard] > a.ActorStoreItem',
                userData: { detailPage: true },
            });
        }
    },
});

await crawler.run();
```

#### What's `Dataset.pushData()`

&#8203;<ApiLink to="core/class/Dataset#pushData">`Dataset.pushData()`</ApiLink> is a helper function that saves data to the default <ApiLink to="core/class/Dataset">`Dataset`</ApiLink>. `Dataset` is a storage designed to hold virtually unlimited amount of data in a format similar to a table. Each time you call `Dataset.pushData()` a new row in the table is created, with the property names serving as column titles.

> Each crawler run has one default `Dataset` so no need to initialize it or create an instance first. It just gets done automatically for you. You can also create named datasets at will.

#### Finding your saved data

It might not be perfectly obvious where the data you saved using the previous command went. Unless you changed the environment variables that Crawlee uses locally, which would suggest that you knew what you were doing, and you didn't need this tutorial anyway, you'll find your data in the `storage` directory:

```
{PROJECT_FOLDER}/storage/datasets/default/
```

The above folder will hold all your saved data in numbered files, as they were pushed into the dataset. Each file represents one invocation of `Dataset.pushData()` or one table row.

### Final touch

It may seem that the data are extracted and the crawler is done, but honestly, this is just the beginning. For the sake of brevity, we've completely omitted error handling, proxies, debug logging, tests, documentation and other stuff that a reliable software should have. The good thing is, **error handling is mostly done by Crawlee itself**, so no worries on that front, unless you need some custom magic.

Anyway, to spark some ideas, let's look at two more things. First, passing an input to the crawler, which will enable you to change the user page you want to scrape without changing the source code itself! And then some refactoring, to show how we reckon is preferable to structure and annotate crawler code.

#### Meet the `INPUT`

`INPUT` is just a convention on how we call the crawler's input. Because there's no magic in crawlers, just features, the `INPUT` is actually nothing more than a key in the default <ApiLink to="core/class/KeyValueStore">`KeyValueStore`</ApiLink>. Also by convention, the `INPUT` is mostly expected to be of `Content-Type: application/json`.

We will not go into `KeyValueStore` details here, but for the sake of `INPUT` you need to remember that there is a function that helps you get it.

```js
const input = await KeyValueStore.getInput();
```

You need to place an `INPUT.json` file in your default key-value store for this to work.

```
{PROJECT_FOLDER}/storage/key_value_stores/default/INPUT.json
```

#### Use `INPUT` to seed your crawler with users

Currently, we're using the full URLs of user pages as sources, but it's quite obvious that we only need the final parameter, the rest of the URL is always the same. Knowing that, we can pass an array of those parameters on `INPUT` and build the URLs dynamically, which would allow us to scrape public actors created by different users without changing the source code. Let's get to it!

For simplicity, let's set the default input in the code directly, as a fallback in case the input is not found in the default `KeyValueStore`:

> We are using the [nullish coalescing operator (`??`)](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Nullish_coalescing_operator), it will use the value on the right hand side only if the left hand side resolves to `null` or `undefined`.

```js
const input = await KeyValueStore.getInput() ?? ['apify'];
```

Next, we will be using the usernames in the input to construct the user URLs, and we'll also pass custom `userData` to the sources. This means that the requests that get created will automatically contain this `userData`.

```js
// ...
const input = await KeyValueStore.getInput() ?? ['apify'];

const sources = input.map((username) => ({
    url: `https://apify.com/${username}`,
    userData: {
        label: 'USER_PAGE',
    },
}));

const requestList = await RequestList.open('users', sources);
// ...
```

The `userData.label` is also a convention that we've been using for quite some time to label different requests. We know that this is a user page URL so we `label` it `USER_PAGE`. This way, we can easily make decisions in the `requestHandler` without having to inspect the URL itself.

We can then refactor the `if` clauses in the `requestHandler` to use the `label` for decision-making. This does not make much sense for a crawler with only two different pages, because a simple `boolean` would suffice, but for pages with multiple different views, it becomes very useful.

#### Structuring the code better

But perhaps we should not stop at refactoring the `if` clauses. There are several ways you can make the crawler look more elegant and - most importantly - easier to reason about and make changes to.

In the following code we've made several changes:
- Split the code into multiple files;
- Replaced `console.log` with the Crawlee logger;
- Added a `getSources()` function to encapsulate `INPUT` consumption;
- Added a `createRouter()` function to make our routing cleaner, without nested `if` clauses;
- Removed the `maxRequestsPerCrawl` limit.

In our `main.js` file, we place the general structure of the crawler:

```js title="main.js"
import { CheerioCrawler, log } from 'crawlee';
import { router } from './routes.js';
import { getSources } from './tools.js';

log.info('Starting crawler.');

log.debug('Setting up crawler.');
const crawler = new CheerioCrawler({
    requestHandler: router,
});

await crawler.addRequests(await getSources());

log.info('Starting the crawl.');
await crawler.run();
log.info('Crawl finished.');
```

Then in a separate `tools.js`, we add our helper functions:

```ts title="tools.js"
import { KeyValueStore, log } from 'crawlee';

export const DEFAULT_USERNAMES = ['apify'];

export async function getSources() {
    log.debug('Getting sources.');
    const input = await KeyValueStore.getInput() ?? DEFAULT_USERNAMES;
    return input.map((username) => ({
        url: `https://apify.com/${username}`,
        label: 'USER_PAGE',
    }));
}
```

And finally our routes in a separate `routes.js` file:

```js title="routes.js"
import { createCheerioRouter, Dataset } from 'crawlee';

export const router = createCheerioRouter();

router.addHandler('USER_PAGE', async ({ enqueueLinks }) => {
    await enqueueLinks({
        selector: 'div[data-test=actorCard] > a.ActorStoreItem',
        label: 'DETAIL',
    });
});

router.addHandler('DETAIL', async ({ $, log, request }) => {
    const urlArr = request.url.split('/').slice(-2);

    log.debug('Scraping results.');
    const results = {
        url: request.url,
        uniqueIdentifier: urlArr.join('/'),
        owner: urlArr[0],
        title: $('header h1').text(),
        description: $('header span.actor-description').text(),
        modifiedDate: new Date(
            Number(
                $('ul.ActorHeader-stats time').attr('datetime'),
            ),
        ),
        runCount: Number(
            $('ul.ActorHeader-stats > li:nth-of-type(3)')
                .text()
                .match(/[\d,]+/)[0]
                .replaceAll(',', ''),
        ),
    };

    log.debug('Pushing data to dataset.');
    await Dataset.pushData(results);
});
```

Let's describe the changes a bit more in detail. We hope that in the end, you can agree that this structure makes the actor more readable and manageable.

#### Splitting your code into multiple files

There's no reason not to split your code into multiple files and keep your logic separate.
Less code in a single file means less code you need to think about at any time, and that's a great thing!

#### Using Crawlee `log` instead of `console.log`

We won't go to great lengths here to talk about `log` object from Crawlee, because you can read it all in the <ApiLink to="core/class/Log">documentation</ApiLink>, but there's just one thing that we need to stress: **log levels**.

`utils.log` enables the usage of different log levels, such as `log.debug`, `log.info` or `log.warning`. It not only makes your log more readable, but it also allows selective turning off of some levels by either calling the `log.setLevel()` function or by setting the `CRAWLEE_LOG_LEVEL` environment variable. This is huge! Because you can now add a lot of debug logs in your actor, which will help you when something goes wrong and turn them on or off with a simple `INPUT` change, or by setting an environment variable.

The punch line? Use `log` exported from `crawlee` instead of `console.log` now and thank us later when something goes wrong!

#### Using a router to structure your crawling

At first, it might seem more readable using just a simple `if / else` statement to select different logic based on the crawled pages, but trust us, it becomes far less impressive when working with more than two different pages, and it definitely starts to fall apart when the logic to handle each page spans tens or hundreds of lines of code.

It's good practice in any programming language to split your logic into bite-sized chunks that are easy to read and reason about. Scrolling through a thousand line long `requestHandler()` where everything interacts with everything and variables can be used everywhere is not a beautiful thing to do and a pain to debug. That's why we prefer the separation of routes into a special file and with large routes, we would even suggest having one file per route.

## Running your crawler in the Cloud

Now that you have your crawler ready, it's the right time to think about where you want to run it. You should already have docker image for the crawler ready, as it was generated by the CLI. To read more about how to run this docker image in the cloud, check out the [Apify Platform guide](../guides/apify-platform).

[//]: # (> TO BE CONTINUED with details on `PlaywrightCrawler` and other features...)
